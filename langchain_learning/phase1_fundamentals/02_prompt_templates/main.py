# !/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
# ----------------------------------------------------------------------------------------------------------------------
from os import getenv
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

load_dotenv()
base_url = getenv("DEEPSEEK_BASE_URL")
api_key = getenv('DEEPSEEK_API_KEY')

model = init_chat_model(
    model='deepseek-chat',
    api_key=api_key,
    max_tokens=50,
)


# 1.ä¸ºä»€ä¹ˆç”¨æç¤ºè¯æ¨¡æ¿ï¼ˆå¯¹æ¯”å­—ç¬¦ä¸²æ‹¼æ¥ï¼‰
def example_1_why_template():
    # å­—ç¬¦ä¸²æ‹¼æ¥
    topic = 'AIåº”ç”¨å¼€å‘'
    difficulty = 'medium'
    prompt_str = f'ä½ æ˜¯ä¸€ä¸ª{difficulty}çº§åˆ«çš„ç¼–ç¨‹å¯¼å¸ˆã€‚è¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Š{topic}ã€‚'
    response_str = model.invoke(prompt_str)
    print(response_str.content)

    # ç®€å•æç¤ºè¯æ¨¡æ¿ PromptTemplate
    template = PromptTemplate.from_template(
        'ä½ æ˜¯ä¸€ä¸ª{difficulty}çº§åˆ«çš„ç¼–ç¨‹å¯¼å¸ˆï¼Œè¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Š{topic}ã€‚'
    )
    prompt = template.format(
        difficulty=difficulty,
        topic=topic,
    )
    response_prompt = model.invoke(prompt)
    print(response_prompt.content)
    '''ğŸ’¡ ä¼˜åŠ¿ï¼š
    1. å¯å¤ç”¨ - åŒä¸€ä¸ªæ¨¡æ¿å¯ä»¥ç”¨äºä¸åŒçš„è¾“å…¥
    2. å¯ç»´æŠ¤ - æ¨¡æ¿å’Œæ•°æ®åˆ†ç¦»ï¼Œæ˜“äºä¿®æ”¹
    3. ç±»å‹å®‰å…¨ - è‡ªåŠ¨éªŒè¯å˜é‡
    4. å¯æµ‹è¯• - æ›´å®¹æ˜“ç¼–å†™æµ‹è¯•ç”¨ä¾‹'''


# 2ï¼šPromptTemplate åŸºç¡€ç”¨æ³•
def example_2_prompt_template_basics():
    # PromptTemplateç”¨äºç®€å•åœºæ™¯

    # 1.from_templateï¼ˆæœ€æ¨èï¼‰

    template_first = PromptTemplate.from_template(
        'å°†ä¸‹åˆ—æ–‡æœ¬ç¿»è¯‘æˆ{language}ï¼š\n{text}'
    )
    prompt_first = template_first.format(
        language='éŸ©è¯­',
        text='ä½ å¥½ï¼Œæˆ‘æ˜¯AIåº”ç”¨å·¥ç¨‹å¸ˆã€‚'
    )
    print(prompt_first)

    res_first = model.invoke(prompt_first)
    print(res_first.content)

    # 2.æ˜¾ç¤ºæŒ‡å®šå˜é‡ï¼ˆè¯­æ³•æ›´ä¸¥æ ¼ï¼‰
    template_second = PromptTemplate(
        input_variables=['product', 'feature'],
        template='ä¸º{product}äº§å“ç¼–å†™ä¸€æ®µå¹¿å‘Šæ ‡è¯­ï¼Œä»¥{feature}ä¸ºæ ¸å¿ƒ,ä»¥{language}æ˜¾ç¤ºã€‚',
    )
    prompt_second = template_second.format(
        product='æ™ºèƒ½æ‰‹è¡¨',
        feature='æŒæ§æ—¶é—´',
        language='ä¸­æ–‡ç¹ä½“'
    )
    print(prompt_second)

    res_second = model.invoke(prompt_second)
    print(res_second.content)
    print(res_second)

    # 3.invokeç›´æ¥ç”Ÿæˆï¼ˆæ›´æ–¹ä¾¿ï¼‰
    template_third = PromptTemplate.from_template(
        "ä¸º{season}å†™ä¸€é¦–{style}è¯—ï¼Œå­—æ•°{count}"
    )

    # invoke ç›´æ¥è¿”å›æ ¼å¼åŒ–åçš„å€¼
    prompt_third = template_third.invoke({
        'season': 'å†¬å¤©',
        'style': 'ç°ä»£',
        'count': '100',
    })
    print(prompt_third)

    res_third = model.invoke(prompt_third)
    print(res_third.content)


# 3.ChatPromptTemplate èŠå¤©æ¶ˆæ¯æ¨¡æ¿
def example_3_chatprompttemplate():
    # ä½¿ç”¨å…ƒç»„æ ¼å¼ï¼ˆæœ€ç®€å•ï¼Œæ¨èï¼‰
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œæ“…é•¿{expertise}ã€‚"),
        ("user", "è¯·ç»™æˆ‘{task}"),
    ])

    messages = template.format_messages(
        role='AIæ•™æˆ',
        expertise='å¤§æ¨¡å‹å¼€å‘',
        task='è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
    )
    print(messages)

    res_chat = model.invoke(messages)
    print(res_chat.content)


# å¤šè½®å¯¹è¯æ¨¡æ¿
def example_4_conversation_template():
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ã€‚{instruction}"),
        ("user", "{question1}"),
        ("assistant", "{answer1}"),  # ç›®å‰æ²¡çœ‹å‡ºæ¥æ€ä¹ˆä½¿ç”¨ï¼Ÿï¼Ÿï¼Ÿ
        ("user", "{question2}")
    ])

    # å¡«å……æ¨¡æ¿
    messages = template.format_messages(
        role="Python ä¸“å®¶",
        instruction="å›ç­”è¦ç®€æ´ã€å‡†ç¡®",
        question1="ä»€ä¹ˆæ˜¯åˆ—è¡¨ï¼Ÿ",
        answer1="åˆ—è¡¨æ˜¯ Python ä¸­çš„æœ‰åºå¯å˜é›†åˆï¼Œç”¨æ–¹æ‹¬å· [] è¡¨ç¤ºã€‚",
        question2="å®ƒå’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"  # åŸºäºä¸Šä¸‹æ–‡çš„é—®é¢˜
    )

    response = model.invoke(messages)
    print(f"\nAI å›å¤ï¼š{response.content}\n")


# ä½¿ç”¨MessagePromptTemplateç±»ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

# â€œæ›´ç»†ç²’åº¦çš„æ§åˆ¶â€æŒ‡çš„æ˜¯èƒ½å¤Ÿæ·±å…¥åˆ°æ¡†æ¶çš„å„ä¸ªç»„ä»¶å’Œæµç¨‹ä¸­ï¼Œè¿›è¡Œå®šåˆ¶åŒ–è°ƒæ•´å’Œå¹²é¢„ã€‚

def example_5_message_prompt_template():
    system_template = SystemMessagePromptTemplate.from_template(
        'ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œ{instruction}'
    )
    human_template = HumanMessagePromptTemplate.from_template(
        'å…³äº{topic}ï¼Œæˆ‘æƒ³çŸ¥é“{question}'
    )

    chat_template = ChatPromptTemplate.from_messages([
        system_template,
        human_template,
    ])

    chat_prompt = chat_template.format_messages(
        role='Pythonè€å¸ˆ',
        instruction='å›ç­”é€šä¿—æ˜“æ‡‚',
        topic='è£…é¥°å™¨',
        question='åŸç†ä¸ç”¨æ³•',
    )

    model_reasoner = init_chat_model(
        model='deepseek-reasoner',
        base_url=base_url,
        api_key=api_key,
        temperature=1.0,
        max_tokens=100,
        timeout=None,  # æ€»æ˜¯è¶…æ—¶
    )

    response_message = model_reasoner.invoke(chat_prompt)
    print(response_message.content)
    for data in response_message:
        print(data)


# éƒ¨åˆ†å˜é‡ï¼ˆPartial Variableï¼‰
def example_6_partial_variable():
    original_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œæ“…é•¿{expertise}ã€‚"),
        ("user", "è¯·ç»™æˆ‘{task}ï¼Œæ¦‚æ‹¬å›ç­”"),
    ])
    print(original_template)

    partially_filled = original_template.partial(
        role='LangChainå­¦ç”Ÿ',
        expertise='AI appå¼€å‘',
    )
    print(partially_filled)

    message_one = partially_filled.format_messages(
        task='ä»‹ç»ç”¨langchainè¿›è¡ŒAIåº”ç”¨å¼€å‘çš„æ­¥éª¤',
    )
    print(message_one)

    res_one = model.invoke(message_one)
    print(res_one.content)

    # å¤ç”¨æ¨¡æ¿ï¼Œä¸åŒçš„ task
    message_two = partially_filled.format_messages(
        task='ä»2026å¹´ä»¥åçš„å‘å±•å‰æ™¯æ€ä¹ˆæ ·'
    )
    res_two = model.invoke(message_two)
    print(res_two.content)


# ä¸LCELé“¾å¼è°ƒç”¨ï¼ˆé¢„è§ˆï¼‰
def example_7_lcel_chains():
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}"),
        ("user", "{input}"),
    ])

    chain = template | model

    res = chain.invoke({
        "role": "å¹½é»˜çš„ç¨‹åºå‘˜",
        "input": "è§£é‡Šä»€ä¹ˆæ˜¯bug"
    })
    print(res.content)


def main():
    try:
        example_1_why_template()
        example_2_prompt_template_basics()
        example_3_chatprompttemplate()
        example_4_conversation_template()
        example_5_message_prompt_template()
        example_6_partial_variable()
        example_7_lcel_chains()
    except Exception as e:
        print(e)


if __name__ == '__main__':
    main()
